# Deep Learning - Analyse de sentiments multi-langues

Pr√©sentation Equipe : 
* Florian Caliz, 
* Aubin Porte, 
* Paul Loublier, 
* Juliette Verlaine.

__Contexte__ : D√©velopper et mettre en place un mod√®le permettant de d√©tecter des sentiments sur du texte, d'abord en anglais et si possible, sur une langue autre.


## Instanciation du projet

* Etape 1 : T√©l√©charger le dataset des donn√©es sources [ici](http://help.sentiment140.com/for-students)
* Etape 2 : Lancer le notebook _transform.ipynb_ pour effectuer le nettoyage et la transformation des donn√©es. Ce script g√©n√©rera un fichier proper_df.csv.
* Etape 3 a. : Soit vous d√©cidez d'entra√Æner le mod√®le, auquel cas, lancer le fichier _main.py_
* Etape 3 b. : Soit vous d√©cidez de lancer directement le mod√®le sur les donn√©es de test t√©l√©charg√©es pr√©c√©demment pour voir comment il se comporte face √† de nouvelles donn√©es. Pour cela, utiliser le fichier _load_model.py_

## Introduction

Ce premier projet de Deep Learning est un projet visant √† mettre en place un mod√®le capable de classifier du texte et de pr√©dire des labels. Sur un jeu de donn√©es pr√© annot√©, le mod√®le doit s'entra√Æner
√† classifier le texte √† savoir s'il est n√©gatif ou positif, et une fois entra√Æn√© avec une bonne accuracy, il doit √™tre capable sur du texte brute, sans annotation, de pr√©dire la valeur du label associ√©. 

## Transformation des donn√©es

### a. Dataset

Le dataset choisi est un dataset qui regroupe des tweet divers et vari√©s, pr√© anot√©s. Le dataset initial est sur la base d'un .tsv. Les diff√©rentes colonnes sont : 

* 0 - La polarit√© du tweet (0 = negative, 2 = neutral, 4 = positive).
* 1 - L'id du tweet (2087).
* 2 - La date du tweet (Sat May 16 23:58:44 UTC 2009).
* 3 - La query (lyx). S'il n'y en a pas, la cellule est remplie par : NO_QUERY.
* 4 - L'utilisateur qui a r√©dig√© le tweet (robotickilldozr).
* 5 - Le contenu du tweet sous la forme de texte (Lyx is cool).

### b. Transformation

Dans un premier temps, il s'agit de s'assurer que nous disposons des donn√©es n√©cessaires pour l'application d'un mod√®le de classification. Pour all√©ger les traitements, il faut aussi s'assurer que nous ne disposons pas de donn√©es qui n'ont pas d'int√©r√™t dans la mise en place de notre mod√®le.
On constate assez rapidement que les colonnes id, date, query et user n'ont aucun impact quand √† la d√©tection de sentiment que fera notre mod√®le. De ce fait, nous les supprimons.

Maintenant, il s'agit de d√©terminer les caract√®res ou string qui pourraient poluer l'application du mod√®le : url, caract√®res sp√©ciaux, citation des utilisateurs (@robotickilldozr) ou encore les #.
On garde la valeur des #thisissocool mais on enl√®ve le symbole # qui pourrait poluer l'analyse qui suivra. Pour ce faire on utilise des regex : 

```
df['text'] = df['text'].str.replace('http\\S+|www.\\S+', '', case=False)
df['text'] = df['text'].str.replace('@\\S+', '', case=False)
```

__Etape de visualisation__ : L'√©tape de visualisation permet de comprendre la r√©partition des classes, ici positif et n√©gatif (pas de classe neutre) mais aussi l'occurence des mots selon les labels. 
Les nuages de mots, par labels, permettent de comprendre si des textes annot√©s comme positifs ont une forte occurence mot positif. Ou si des mots √† forte occurence sont pr√©sents mais totalement neutre et du coup, sans impact sur l'apprentissage du mod√®le. 

__Application de fonctions de nettoyages__ : 

```
def cleanHtml(sentence)
def cleanPunc(sentence)
def keepAlpha(sentence)
```

On nettoie ici toute trace de HTML, tout signe de ponctuation et enfin on applique la mise en minuscule de tous les caract√®res afin d'applanir le texte. 

__Lemmatization et Tokenization__ :

Le processus de ¬´ lemmatisation ¬ª consiste √† repr√©senter les mots (ou ¬´ lemmes ¬ª üòâ) sous leur forme canonique. Par exemple pour un verbe, ce sera son infinitif. Pour un nom, son masculin singulier. L'id√©e √©tant encore une fois de ne conserver que le sens des mots utilis√©s dans le corpus.
La tokenisation est l'acte de d√©composer une s√©quence de cha√Ænes en morceaux tels que des mots, des mots-cl√©s, des phrases, des symboles et d'autres √©l√©ments appel√©s jetons. Les jetons peuvent √™tre des mots, des phrases ou m√™me des phrases enti√®res. Dans le processus de tokenisation, certains caract√®res comme les signes de ponctuation sont supprim√©s. Les jetons deviennent l'entr√©e d'un autre processus comme l'analyse et l'exploration de texte.

```
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()
def lemmatize_text(text):
    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]
    proper_df['quote_lemmatizer'] = proper_df.quote.apply(lemmatize_text)
```

## Mod√®le

## Critique du mod√®le

## Conclusion

L'analyse de sentiment, lorsqu'elle comporte deux classes, ici negative et positive n'est pas un cas d'usage qui n√©cessite obligatoirement d'utiliser des mod√®les de Deep Learning. 
En effet, l'entra√Ænement est co√ªteux et les r√©sultats escompt√©s ne sont pas toujours √† la hauteur des attentes. 

Dans le cadre de ce projet, ayant rencontr√©s quelques probl√®mes, nous avons pu tester un mod√®le de Machine Learning : Naive Bayes qui remontait quasiment les m√™mes r√©sultats pour un temps d'entra√Ænement bien moins long
et co√ªteux en terme de performance machine. Il est donc important de bien comprendre les domaines d'application dans lequel il fait sens d'utiliser du deep learning et dans lequel il fait sens d'utiliser du machine learning. Tout d√©pend de la nature des donn√©es en entr√©e, et ce n'est pas par ce que c'est du Deep Learning que c'est pour autant magique :) ! 