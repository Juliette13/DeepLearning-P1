# Deep Learning - Analyse de sentiments multi-langues

Pr√©sentation Equipe : 
* Florian Calliz, 
* Aubin Porte, 
* Paul Loublier et 
* Juliette Verlaine.

__Contexte__ : D√©velopper et mettre en place un mod√®le permettant de d√©tecter des sentiments sur du texte, d'abord en anglais et si possible, sur une langue autre.


## Instanciation du projet

* Etape 1 : T√©l√©charger le dataset des donn√©es sources [ici](http://help.sentiment140.com/for-students)
* Etape 2 : Lancer le notebook _transform.ipynb_ pour effectuer le nettoyage et la transformation des donn√©es. Ce script g√©n√©rera un fichier proper_df.csv.
* Etape 3 a. : Soit vous d√©cidez d'entra√Æner le mod√®le, auquel cas, lancer le fichier _main.py_
* Etape 3 b. : Soit vous d√©cidez de lancer directement le mod√®le sur les donn√©es de test t√©l√©charg√©es pr√©c√©demment pour voir comment il se comporte face √† de nouvelles donn√©es. Pour cela, utiliser le fichier _load_model.py_

## Introduction

## Transformation des donn√©es

### a. Dataset

Le dataset choisi est un dataset qui regroupe des tweet divers et vari√©s, pr√© anot√©s. Le dataset initial est sur la base d'un .tsv. Les diff√©rentes colonnes sont : 

* 0 - La polarit√© du tweet (0 = negative, 2 = neutral, 4 = positive).
* 1 - L'id du tweet (2087).
* 2 - La date du tweet (Sat May 16 23:58:44 UTC 2009).
* 3 - La query (lyx). S'il n'y en a pas, la cellule est remplie par : NO_QUERY.
* 4 - L'utilisateur qui a r√©dig√© le tweet (robotickilldozr).
* 5 - Le contenu du tweet sous la forme de texte (Lyx is cool).

### b. Transformation

Dans un premier temps, il s'agit de s'assurer que nous disposons des donn√©es n√©cessaires pour l'application d'un mod√®le de classification. Pour all√©ger les traitements, il faut aussi s'assurer que nous ne disposons pas de donn√©es qui n'ont pas d'int√©r√™t dans la mise en place de notre mod√®le.
On constate assez rapidement que les colonnes id, date, query et user n'ont aucun impact quand √† la d√©tection de sentiment que fera notre mod√®le. De ce fait, nous les supprimons.

Maintenant, il s'agit de d√©terminer les caract√®res ou string qui pourraient poluer l'application du mod√®le : url, caract√®res sp√©ciaux, citation des utilisateurs (@robotickilldozr) ou encore les #.
On garde la valeur des #thisissocool mais on enl√®ve le symbole # qui pourrait poluer l'analyse qui suivra. Pour ce faire on utilise des regex : 

```
df['text'] = df['text'].str.replace('http\\S+|www.\\S+', '', case=False)
df['text'] = df['text'].str.replace('@\\S+', '', case=False)
```

__Etape de visualisation__ : L'√©tape de visualisation permet de comprendre la r√©partition des classes, ici positif et n√©gatif (pas de classe neutre) mais aussi l'occurence des mots selon les labels. 
Les nuages de mots, par labels, permettent de comprendre si des textes annot√©s comme positifs ont une forte occurence mot positif. Ou si des mots √† forte occurence sont pr√©sents mais totalement neutre et du coup, sans impact sur l'apprentissage du mod√®le. 

__Application de fonctions de nettoyages__ : 

```
def cleanHtml(sentence)
def cleanPunc(sentence)
def keepAlpha(sentence)
```

On nettoie ici toute trace de HTML, tout signe de ponctuation et enfin on applique la mise en minuscule de tous les caract√®res afin d'applanir le texte. 

__Lemmatization et Tokenization__ :

Le processus de ¬´ lemmatisation ¬ª consiste √† repr√©senter les mots (ou ¬´ lemmes ¬ª üòâ) sous leur forme canonique. Par exemple pour un verbe, ce sera son infinitif. Pour un nom, son masculin singulier. L'id√©e √©tant encore une fois de ne conserver que le sens des mots utilis√©s dans le corpus.
La tokenisation est l'acte de d√©composer une s√©quence de cha√Ænes en morceaux tels que des mots, des mots-cl√©s, des phrases, des symboles et d'autres √©l√©ments appel√©s jetons. Les jetons peuvent √™tre des mots, des phrases ou m√™me des phrases enti√®res. Dans le processus de tokenisation, certains caract√®res comme les signes de ponctuation sont supprim√©s. Les jetons deviennent l'entr√©e d'un autre processus comme l'analyse et l'exploration de texte.

```
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()
def lemmatize_text(text):
    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]
    proper_df['quote_lemmatizer'] = proper_df.quote.apply(lemmatize_text)
```

## Mod√®le

## Critique du mod√®le

## Conclusion